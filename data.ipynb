{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from model_arch import *\n",
    "import csv, sys, pdb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rigid_accel2 = \"/home/togzhan/sample/data/rigid_accel2.csv\"\n",
    "file_rigid_accel = \"/home/togzhan/sample/data/rigid_accel.csv\"\n",
    "file_rigid_pressure = \"/home/togzhan/sample/data/rigid_pressure.csv\"\n",
    "file_rigid_imu = \"/home/togzhan/sample/data/rigid_imu.csv\"\n",
    "\n",
    "\n",
    "file_soft_accel2 = \"/home/togzhan/sample/data/soft_accel2.csv\"\n",
    "file_soft_accel = \"/home/togzhan/sample/data/soft_accel.csv\"\n",
    "file_soft_pressure = \"/home/togzhan/sample/data/soft_pressure.csv\"\n",
    "file_soft_imu = \"/home/togzhan/sample/data/soft_imu.csv\"\n",
    "\n",
    "df_accel2_rigid = pd.read_csv(file_rigid_accel2, sep = ',', header = None)\n",
    "df_accel_rigid = pd.read_csv(file_rigid_accel, sep = ',', header = None)\n",
    "df_pressure_rigid = pd.read_csv(file_rigid_pressure, sep = ',', header = None)\n",
    "df_imu_rigid = pd.read_csv(file_rigid_imu, sep = ',', header = None)\n",
    "\n",
    "df_accel2_soft = pd.read_csv(file_soft_accel2, sep = ',', header = None)\n",
    "df_accel_soft = pd.read_csv(file_soft_accel, sep = ',', header = None)\n",
    "df_pressure_soft = pd.read_csv(file_soft_pressure, sep = ',', header = None)\n",
    "df_imu_soft = pd.read_csv(file_soft_imu, sep = ',', header = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = \"/home/togzhan/sample/data/data/df_train4.csv\"\n",
    "dfeval = \"/home/togzhan/sample/data/data/df_eval4.csv\"\n",
    "\n",
    "df_train = pd.read_csv(dftrain, sep = ',', header = None)\n",
    "df_eval = pd.read_csv(dfeval, sep = ',', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "def normalize(data):\n",
    "    m = data.max()\n",
    "    data = pd.DataFrame(data)\n",
    "#     df_max = [420, 420, 420, 418, 418, 450, 432, 432, 432, 390, 472, 480, 480, 480, 435, 433, 501, 405, 400, 400, 400 ,400, 370, 455 ,444, 310, 445]\n",
    "    df_max = [450]\n",
    "    df_max = pd.DataFrame(df_max)\n",
    "    df_min = 0;\n",
    "    p = pd.DataFrame()\n",
    "    k = []\n",
    "    for i in range(27):\n",
    "        k = np.divide(data.values[:,i].astype('float'), df_max.values[0].astype('float'))\n",
    "        k = pd.DataFrame(k)    \n",
    "        p = pd.concat([p, k], axis = 1)\n",
    "        \n",
    "    data = p\n",
    "    data = pd.DataFrame(p)\n",
    "    return data\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_the_data(df_accel2, df_pressure, df_imu, inp):\n",
    "    df_pressure_n = normalize(df_pressure)\n",
    "    dP = pd.DataFrame()\n",
    "    dA = pd.DataFrame()\n",
    "    dI = pd.DataFrame()\n",
    "    D = pd.DataFrame()\n",
    "    data = pd.DataFrame()\n",
    "    df_accelT = df_accel2.transpose()\n",
    "    df_pressure_nT = df_pressure_n.transpose()\n",
    "    df_imuT = df_imu.transpose()\n",
    "    for i in range(50):\n",
    "        P = pd.DataFrame()\n",
    "        A = pd.DataFrame()\n",
    "        I = pd.DataFrame()\n",
    "        for j in range(27):\n",
    "            p = []\n",
    "            p = df_pressure_nT.values[j, i*744:(i+1)*744]\n",
    "            P = pd.concat([P, pd.DataFrame(p)], axis = 0, ignore_index=True, sort = False )\n",
    "        for j in range(200):\n",
    "            a = []\n",
    "            a = df_accelT.values[j, i*744:(i+1)*744]\n",
    "            A = pd.concat([A, pd.DataFrame(a)], axis = 0, ignore_index=True, sort = False)\n",
    "        for j in range(18):\n",
    "            ii = []\n",
    "            ii = df_imuT.values[j, i*744:(i+1)*744]\n",
    "            I = pd.concat([I, pd.DataFrame(ii)], axis = 0, ignore_index=True, sort = False)\n",
    "        \n",
    "        D = pd.concat([P, A, I], axis = 0)\n",
    "#         print I\n",
    "#         print D\n",
    "#         print A\n",
    "        data = pd.concat([data, D], axis = 1)\n",
    "#         dP = pd.concat([dP, P], axis = 1)\n",
    "#         dA = pd.concat([dA, A], axis = 1)\n",
    "#         dI = pd.concat([dI, I], axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bilabel = []\n",
    "for i in range(100):\n",
    "    if(i<50):\n",
    "        df_bilabel.append(0)\n",
    "    else:\n",
    "        df_bilabel.append(1)\n",
    "        \n",
    "df_bilabel = pd.DataFrame(df_bilabel, index=None)\n",
    "\n",
    "k = 0\n",
    "labels = []\n",
    "for i in range(100):\n",
    "    if(np.mod(i,5) == 0):\n",
    "        k = k +1\n",
    "    labels.append(k)        \n",
    "df_label = pd.DataFrame(labels, index=None)\n",
    "\n",
    "# labels - 1st number 0 or 1 0 - rigid 1 - soft, second number defines the object\n",
    "df_newlabel = df_bilabel.astype(str) + \"_\" + df_label.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_bilabel.shape\n",
    "print df_data.shape\n",
    "a = pd.DataFrame()\n",
    "a = pd.concat([df_bilabel, df_label], axis = 1, sort=False)\n",
    "df_data =  df_data.reset_index(drop=True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(path_or_buf=\"/home/togzhan/sample/data/data/df_train4.csv\", index=False)\n",
    "df_eval.to_csv(path_or_buf=\"/home/togzhan/sample/data/data/df_eval4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rigid = pd.DataFrame()\n",
    "data_soft = pd.DataFrame()\n",
    "\n",
    "data_rigid = combine_the_data(df_accel2_rigid, df_pressure_rigid, df_imu_rigid, 1)\n",
    "\n",
    "data_soft = combine_the_data(df_accel2_soft, df_pressure_soft, df_imu_soft, 1)\n",
    "\n",
    "data_rigid = data_rigid.transpose()\n",
    "data_soft = data_soft.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([data_rigid, data_soft], axis = 0, sort = False)\n",
    "df_data.reset_index(drop=True, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_l = pd.DataFrame()\n",
    "df_data_l = pd.concat([df_data, df_bilabel], axis = 1, sort = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_eval = train_test_split(df_data_l, test_size=1.0/3, random_state=777, stratify=df_newlabel.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_train.shape\n",
    "\n",
    "label_train = df_train.values[:, a[1]-1]\n",
    "label_eval = df_eval.values[:, a[1]-1]\n",
    "df_train_e = pd.DataFrame(df_train.values[:, 1:a[1]-18*744-1])\n",
    "df_eval_e = pd.DataFrame(df_eval.values[:, 1:a[1]-18*744-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = \"/home/togzhan/sample/data/data/biclass_adaboost1_max_depth150_n_estimators\"\n",
    "model = torch.load(m)-\n",
    "model.predict(test_x[4].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### LOAD previously saved and shuffled data .\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from model_arch import *\n",
    "import csv, sys, pdb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_train4 = \"/home/togzhan/sample/data/data/df_train4.csv\"\n",
    "df_eval4 = \"/home/togzhan/sample/data/data/df_eval4.csv\"\n",
    "\n",
    "df_train = pd.read_csv(df_train4, sep = ',', header = None)\n",
    "df_eval = pd.read_csv(df_eval4, sep = ',', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x\t= torch.FloatTensor(df_train.values[:,:-1].astype('float64'))\n",
    "test_x = torch.FloatTensor(df_eval.values[:,:-1].astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.FloatTensor(df_train.values[:,-1].astype('float64'))\n",
    "test_y = torch.FloatTensor(df_eval.values[:,-1].astype('float64'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out, drop=0.0):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear1_1 = torch.nn.Linear(H, H)\n",
    "    self.linear1_2 = torch.nn.Linear(H, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    self.drop1   = torch.nn.Dropout(p=drop)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary operators on Tensors.\n",
    "    \"\"\"\n",
    "    h_relu = self.drop1(self.linear1(x)).clamp(min=0)\n",
    "    h_relu = self.drop1(self.linear1_1(h_relu)).clamp(min=0)\n",
    "    h_relu = self.drop1(self.linear1_2(h_relu)).clamp(min=0)\n",
    "    y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "  def __init__(self, H_in, W_in, D_out, ch_out, H, kernel_w=3, stride_size=1, drop=0.0):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate Convlolutional and one Linear modules and assign them as member variables.\n",
    "    \"\"\"\n",
    "    super(ConvNet, self).__init__()\n",
    "    self.conv = torch.nn.Sequential(\n",
    "                  torch.nn.Conv2d(1, ch_out, (H_in, kernel_w), stride=stride_size),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Conv2d(ch_out, ch_out, (H_in, kernel_w), stride=stride_size),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Conv2d(ch_out, ch_out, (H_in, kernel_w), stride=stride_size),\n",
    "                  torch.nn.ReLU()\n",
    "                )\n",
    "    W_out = ((W_in - kernel_w) // stride_size + 1)\n",
    "    W_out = ((W_out - kernel_w) // stride_size + 1)\n",
    "    W_out = ch_out * ((W_out - kernel_w) // stride_size + 1)\n",
    "    #pdb.set_trace()\n",
    "    #self.linear1 = torch.nn.Linear(W_out, D_out)\n",
    "    self.linear1 = torch.nn.Linear(W_out, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    self.drop1   = torch.nn.Dropout(p=drop)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary operators on Tensors.\n",
    "    \"\"\"\n",
    "    (b,w) = x.shape\n",
    "    h_cnn = self.drop1(self.conv(x.view(b,1,1,w)))\n",
    "    b, c, h, w = h_cnn.size()\n",
    "    #pdb.set_trace()\n",
    "    h_relu = self.drop1(self.linear1(h_cnn.view(b,-1))).clamp(min=0)\n",
    "    y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Model\n",
    "dtype = torch.float\n",
    "epochs = 5000\n",
    "N = 16 #batch size\n",
    "D_in = train_x.shape[1]\n",
    "D_out = 1\n",
    "drop = 0.4\n",
    "#FFN Model\n",
    "H = 512 #hidden size\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "ch_out = 4\n",
    "kernel_w = 16\n",
    "stride = 4\n",
    "###################################################################################################\n",
    "\n",
    "W_in = train_x.shape[1]\n",
    "model = ConvNet(1, W_in, D_out, ch_out, H, kernel_w, stride, drop).to(device)\n",
    "#model_path = \"./model/biclass_cnn_l1_h\"+str(H)+\"_bs\"+str(N)+\"_c\"+str(ch_out)+\"_k\"+str(kernel_w)+\"_s\"+str(stride)+\"_drop\"+str(drop)+\"_epoch\"+str(epochs)+\"_accX\"\n",
    "model_path = \"/home/togzhan/sample/data/data/models/biclass_cnn_l1_h\"+str(H)+\"_bs\"+str(N)+\"_c\"+str(ch_out)+\"_k\"+str(kernel_w)+\"_s\"+str(stride)+\"_drop\"+str(drop)+\"_epoch\"+str(epochs)+\"_all\"\n",
    "print(model)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "####################################################################################################\n",
    "#Training\n",
    "####################################################################################################\n",
    "dev_acc_max=0\n",
    "epoch_max=0\n",
    "for e in range(epochs):\n",
    "\ttotal_loss = 0\n",
    "\t#for x, y in zip(train_x, train_y):\n",
    "\tfor i in range(0, train_x.shape[0], N):\n",
    "\t\tif i+N >= train_x.shape[0]:\n",
    "\t\t\tx = train_x[i:]\n",
    "\t\t\ty = train_y[i:]\n",
    "\t\telse:\n",
    "\t\t\tx = train_x[i:i+N]\n",
    "\t\t\ty = train_y[i:i+N]\n",
    "\t\ty_pred = model(x.to(device))\n",
    "\t\tloss = loss_fn(y_pred, y.view(-1,1).to(device))\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item()\n",
    "\tprint(\"Epoch {}/{}, Train Loss: {:.3f}\".format(e+1, epochs, total_loss))\n",
    "torch.save(model, model_path)\n",
    "####################################################################################################\n",
    "#Evaluation\n",
    "####################################################################################################\n",
    "with torch.no_grad():\n",
    "  pdb.set_trace()\n",
    "  model = torch.load(model_path)\n",
    "  model.eval().to(device)\n",
    "  output = model(test_x.to(device))\n",
    "  output = (output>0.5).float()\n",
    "  acc    = accuracy_score(test_y, output.cpu())\n",
    "  print(\"Test Accuracy: {:.3f}\".format(acc))\n",
    "  print(model)\n",
    "\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(path_or_buf=\"/home/togzhan/sample/data/data/df_data.csv\", index=False) \n",
    "\n",
    "df_train.to_csv(path_or_buf=\"/home/togzhan/sample/data/data/df_train3.csv\", index=False) \n",
    "\n",
    "df_eval.to_csv(path_or_buf=\"/home/togzhan/sample/data/data/df_eval3.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  4],\n",
       "       [ 0, 17]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaboost\n",
    "mx = 5\n",
    "ns = 200 # 100 - 94% accuracy\n",
    "lr = 0.6\n",
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth = mx),\n",
    "    n_estimators=ns,\n",
    "    learning_rate = lr\n",
    ")\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "predictions = classifier.predict(test_x)\n",
    "\n",
    "confusion_matrix(test_y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/togzhan/anaconda3/envs/python2env/lib/python2.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "svc=SVC(probability=True, kernel=\"sigmoid\")\n",
    "# Adaboost\n",
    "mx = 5\n",
    "ns = 200 # 100 - 94% accuracy\n",
    "lr = 0.6\n",
    "classifier = AdaBoostClassifier(\n",
    "    base_estimator=svc,\n",
    "#     DecisionTreeClassifier(max_depth = mx),\n",
    "    n_estimators=ns,robotics123\n",
    "    m\n",
    "    learning_rate = lr\n",
    ")\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "predictions = classifier.predict(test_x)\n",
    "\n",
    "confusion_matrix(test_y, predictions)\n",
    "\n",
    "a = metrics.accuracy_score(test_y, predictions)\n",
    "print(\"Accuracy:\", a)\n",
    "a = np.string_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1fc25cd1b4d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_errors_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "classifier.estimator_errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method AdaBoostClassifier.staged_score of AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=200, random_state=None)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.staged_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.8857142857142857)\n"
     ]
    }
   ],
   "source": [
    "a = metrics.accuracy_score(test_y, predictions)\n",
    "print(\"Accuracy:\", a)\n",
    "a = np.string_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/togzhan/sample/data/data/biclass_adaboost\"+str(mx)+\"_max_depth\"+str(ns)+\"_n_estimators\"+a+\"acc\"\n",
    "torch.save(classifier, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
